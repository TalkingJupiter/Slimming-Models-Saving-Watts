{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064526c6",
   "metadata": {},
   "source": [
    "$$\n",
    "Eff_{overal} = \n",
    "\\frac{1}{K}\n",
    "\\sum_{i=1}^K\n",
    "\\left[\n",
    "    OM_{perf} (S_i | R)\n",
    "    \\cdot\n",
    "    \\left(\n",
    "        \\frac{E_A^T}{E_A^{S_i}}\n",
    "    \\right)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11322998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be6b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEACHER_EVAL_PATH = Path(\"../../logs/ept_logs/teacher_base_parallel/eval_results_2025-11-26T11-15-36.434403.json\")  # <--- EDIT\n",
    "TEACHER_EPT_JSON = Path(\"../../eval/ept/benchmark/results/ept_teacher_23901.json\")\n",
    "\n",
    "# Relation KD\n",
    "REL_EVAL_DIR  = Path(\"../../results/relation/\")   # EDIT\n",
    "REL_EPT_DIR = Path(\"../../eval/ept/benchmark/relation_23898/\")\n",
    "\n",
    "# Response KD\n",
    "RESP_EVAL_DIR  = Path(\"../../results/response/\")  # EDIT\n",
    "RESP_EPT_DIR = Path(\"../../eval/ept/benchmark/response_23899/\")\n",
    "\n",
    "# Feature KD\n",
    "FEAT_EVAL_DIR  = Path(\"../../results/feature/\")   # EDIT\n",
    "FEAT_EPT_DIR = Path(\"../../eval/ept/benchmark/feature_23897/\")\n",
    "\n",
    "# Tasks used in OM_perf and Eff_overall\n",
    "TASKS = [\"arc_challenge\", \"bbh\", \"hellaswag\", \"mmlu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89620795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87e6eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_json_maybe_zip(path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Load an LM-Eval JSON.\n",
    "\n",
    "    - If 'path' is a .json: read it directly.\n",
    "    - If 'path' is a .zip: open the zip, read the first .json inside.\n",
    "    \"\"\"\n",
    "    if path.suffix == \".json\":\n",
    "        with path.open(\"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    if path.suffix == \".zip\":\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            # take the first JSON file inside\n",
    "            json_names = [n for n in z.namelist() if n.endswith(\".json\")]\n",
    "            if not json_names:\n",
    "                raise FileNotFoundError(f\"No JSON file found inside {path}\")\n",
    "            with z.open(json_names[0], \"r\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "    raise ValueError(f\"Unsupported eval file type: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd076ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_task_scores(eval_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the 4 main task scores into a flat dict:\n",
    "\n",
    "      - arc_challenge: acc_norm\n",
    "      - bbh:           acc\n",
    "      - hellaswag:     acc_norm\n",
    "      - mmlu:          acc\n",
    "\n",
    "    This assumes the LM-Eval JSON structure where metrics live under\n",
    "    eval_dict[\"results\"][task][metric_name][\"value\"] or [\"acc\"] style.\n",
    "    Adjust if your schema is slightly different.\n",
    "    \"\"\"\n",
    "    results = eval_dict.get(\"results\", eval_dict)  # be a bit robust\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # ARC-Challenge\n",
    "    arc = results[\"arc_challenge\"]\n",
    "    if isinstance(arc, dict):\n",
    "        # LM-Eval usually: {\"acc\": ..., \"acc_norm\": ...}\n",
    "        out[\"arc_challenge\"] = arc.get(\"acc_norm\", arc.get(\"acc\"))\n",
    "\n",
    "    # BBH\n",
    "    bbh = results[\"bbh\"]\n",
    "    out[\"bbh\"] = bbh.get(\"acc\", bbh.get(\"accuracy\", None))\n",
    "\n",
    "    # HellaSwag\n",
    "    hs = results[\"hellaswag\"]\n",
    "    out[\"hellaswag\"] = hs.get(\"acc_norm\", hs.get(\"acc\"))\n",
    "\n",
    "    # MMLU\n",
    "    mmlu = results[\"mmlu\"]\n",
    "    out[\"mmlu\"] = mmlu.get(\"acc\", mmlu.get(\"accuracy\", None))\n",
    "\n",
    "    # Sanity check\n",
    "    missing = [k for k, v in out.items() if v is None]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing metrics for tasks: {missing}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2301be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalizer(eval_dict: dict) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Try to automatically get a normalizer for energy:\n",
    "\n",
    "      1) Prefer token-based fields (per-token metric):\n",
    "         total_generated_tokens / total_tokens / num_eval_tokens / num_generated_tokens\n",
    "\n",
    "      2) Fallback: total_evaluation_time_seconds (per-second metric)\n",
    "\n",
    "    Returns:\n",
    "        (value, kind) where kind is \"tokens\" or \"seconds\".\n",
    "    \"\"\"\n",
    "    token_keys = [\n",
    "        \"total_generated_tokens\",\n",
    "        \"total_tokens\",\n",
    "        \"num_eval_tokens\",\n",
    "        \"num_generated_tokens\",\n",
    "    ]\n",
    "\n",
    "    for k in token_keys:\n",
    "        if k in eval_dict:\n",
    "            return float(eval_dict[k]), \"tokens\"\n",
    "\n",
    "    if \"total_evaluation_time_seconds\" in eval_dict:\n",
    "        return float(eval_dict[\"total_evaluation_time_seconds\"]), \"seconds\"\n",
    "\n",
    "    raise KeyError(\n",
    "        \"Could not find token count or total_evaluation_time_seconds in eval JSON.\\n\"\n",
    "        \"Add one of: total_generated_tokens / total_tokens / num_eval_tokens / \"\n",
    "        \"num_generated_tokens / total_evaluation_time_seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ad50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_energy_mj_series(jsonl_path: Path, gpu_index: int | None = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Load cumulative energy_mJ from a monitor.py JSONL file.\n",
    "    If gpu_index is None: sum across all GPUs.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "\n",
    "    with jsonl_path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            d = json.loads(line)\n",
    "            gpus = d[\"gpus\"]\n",
    "\n",
    "            if gpu_index is None:\n",
    "                total_mJ = sum(g[\"energy_mJ\"] for g in gpus)\n",
    "                vals.append(total_mJ)\n",
    "            else:\n",
    "                for g in gpus:\n",
    "                    if g[\"gpu_index\"] == gpu_index:\n",
    "                        vals.append(g[\"energy_mJ\"])\n",
    "                        break\n",
    "\n",
    "    return pd.Series(vals, name=\"energy_mJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ded6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_E_run_J(energy_mJ_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute total energy for a run in Joules\n",
    "    from cumulative energy_mJ series.\n",
    "    \"\"\"\n",
    "    if energy_mJ_series.empty:\n",
    "        return 0.0\n",
    "    return (energy_mJ_series.iloc[-1] - energy_mJ_series.iloc[0]) / 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9649488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_OM_perf(student_scores: dict,\n",
    "                    teacher_scores: dict,\n",
    "                    tasks = TASKS) -> float:\n",
    "    \"\"\"\n",
    "    OM_perf(S|T) = sqrt( (1/M) * sum_t (A_t^S / A_t^T)^2 )\n",
    "\n",
    "    Captures how well the student retains the teacher's ability across M tasks.\n",
    "    \"\"\"\n",
    "    ratios_sq = []\n",
    "    for t in tasks:\n",
    "        A_s = student_scores[t]\n",
    "        A_t = teacher_scores[t]\n",
    "        ratios_sq.append((A_s / A_t) ** 2)\n",
    "\n",
    "    M = len(tasks)\n",
    "    return math.sqrt(sum(ratios_sq) / M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93d67c97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Missing metrics for tasks: ['arc_challenge', 'bbh', 'hellaswag', 'mmlu']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---- Load teacher eval (from JSON or ZIP) ----\u001b[39;00m\n\u001b[1;32m      2\u001b[0m teacher_eval_dict \u001b[38;5;241m=\u001b[39m load_eval_json_maybe_zip(TEACHER_EVAL_PATH)\n\u001b[0;32m----> 3\u001b[0m teacher_scores    \u001b[38;5;241m=\u001b[39m \u001b[43mextract_main_task_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_eval_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m teacher_norm_val, teacher_norm_kind \u001b[38;5;241m=\u001b[39m get_normalizer(teacher_eval_dict)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, teacher_scores)\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mextract_main_task_scores\u001b[0;34m(eval_dict)\u001b[0m\n\u001b[1;32m     37\u001b[0m missing \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing metrics for tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Missing metrics for tasks: ['arc_challenge', 'bbh', 'hellaswag', 'mmlu']\""
     ]
    }
   ],
   "source": [
    "# ---- Load teacher eval (from JSON or ZIP) ----\n",
    "teacher_eval_dict = load_eval_json_maybe_zip(TEACHER_EVAL_PATH)\n",
    "teacher_scores    = extract_main_task_scores(teacher_eval_dict)\n",
    "teacher_norm_val, teacher_norm_kind = get_normalizer(teacher_eval_dict)\n",
    "\n",
    "print(\"Teacher scores:\", teacher_scores)\n",
    "print(\"Teacher normalizer:\", teacher_norm_val, f\"({teacher_norm_kind})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a81257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load teacher EPT benchmark and compute energy per token ----\n",
    "# We now use the EPT benchmark summary JSON (token file) instead of raw monitor.py telemetry\n",
    "# for the Eff_overall calculation.\n",
    "# TEACHER_EPT_JSON should point to a single JSON file like:\n",
    "# {\n",
    "#   \"E_run_J\": ...,\n",
    "#   \"T_in\": ...,\n",
    "#   \"T_out\": ...,\n",
    "#   \"EPT_in_J_per_tok\": ...,\n",
    "#   \"EPT_out_J_per_tok\": ...,\n",
    "#   \"EPT_total_J_per_tok\": ...\n",
    "# }\n",
    "\n",
    "with TEACHER_EPT_JSON.open(\"r\") as f:\n",
    "    teacher_ept = json.load(f)\n",
    "\n",
    "teacher_E_run_J = teacher_ept[\"E_run_J\"]\n",
    "teacher_T_in    = teacher_ept.get(\"T_in\")\n",
    "teacher_T_out   = teacher_ept.get(\"T_out\")\n",
    "\n",
    "teacher_EPT_in_J_per_tok   = teacher_ept.get(\"EPT_in_J_per_tok\")\n",
    "teacher_EPT_out_J_per_tok  = teacher_ept.get(\"EPT_out_J_per_tok\")\n",
    "teacher_E_per_unit_J       = teacher_ept[\"EPT_total_J_per_tok\"]  # J per token from EPT/token file\n",
    "\n",
    "print(f\"Teacher E_run_J (EPT): {teacher_E_run_J:.3f} J\")\n",
    "if teacher_T_in is not None and teacher_T_out is not None:\n",
    "    print(f\"Teacher tokens (EPT run): T_in={teacher_T_in}, T_out={teacher_T_out}, \"\n",
    "          f\"total={teacher_T_in + teacher_T_out}\")\n",
    "print(f\"Teacher EPT_total_J_per_tok: {teacher_E_per_unit_J:.6e} J/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_from_eval(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract a stable experiment tag from an LM-Eval JSON filename.\n",
    "\n",
    "    Example:\n",
    "      23797_harness_meta-llama_Llama-3.1-8B-Instruct__20251113_2057_RelB_1n_20251123_185443_2025-11-24T07-44-58.217405.json\n",
    "    -> tag: 20251113_2057_RelB_1n\n",
    "    \"\"\"\n",
    "    stem = path.stem\n",
    "    if \"__\" in stem:\n",
    "        suffix = stem.split(\"__\", 1)[1]\n",
    "    else:\n",
    "        suffix = stem\n",
    "\n",
    "    parts = suffix.split(\"_\")\n",
    "    if len(parts) < 4:\n",
    "        raise ValueError(f\"Cannot extract tag from eval filename: {path.name}\")\n",
    "    tag = \"_\".join(parts[:4])\n",
    "    return tag\n",
    "\n",
    "\n",
    "def extract_tag_from_ept(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract the same experiment tag from an EPT JSON filename.\n",
    "\n",
    "    Example:\n",
    "      ept_REL_1_20251113_2057_RelB_1n_23898.json\n",
    "    -> tag: 20251113_2057_RelB_1n\n",
    "    \"\"\"\n",
    "    stem = path.stem\n",
    "    parts = stem.split(\"_\")\n",
    "\n",
    "    date_idx = None\n",
    "    for i, p in enumerate(parts):\n",
    "        if len(p) == 8 and p.isdigit():\n",
    "            date_idx = i\n",
    "            break\n",
    "\n",
    "    if date_idx is None or len(parts) < date_idx + 4:\n",
    "        raise ValueError(f\"Cannot extract tag from EPT filename: {path.name}\")\n",
    "\n",
    "    tag_parts = parts[date_idx:date_idx + 4]\n",
    "    tag = \"_\".join(tag_parts)\n",
    "    return tag\n",
    "\n",
    "\n",
    "def build_kd_df(eval_dir: Path, ept_dir: Path, kd_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a dataframe of OM_perf, energy-per-token, and Eff_model\n",
    "    for all student models in a given KD family (relation / response / feature).\n",
    "\n",
    "    Matching is done via a shared experiment TAG, e.g. '20251113_2057_RelB_1n',\n",
    "    which appears in BOTH the eval filename and the EPT filename, even though\n",
    "    the full names differ.\n",
    "    \"\"\"\n",
    "    eval_files = sorted(eval_dir.glob(\"*.json\"))\n",
    "    if not eval_files:\n",
    "        print(f\"[WARN] No eval JSON files found in {eval_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ept_files = sorted(ept_dir.glob(\"*.json\"))\n",
    "    if not ept_files:\n",
    "        print(f\"[WARN] No EPT JSON files found in {ept_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Map tag -> EPT path\n",
    "    ept_by_tag: dict[str, Path] = {}\n",
    "    for ept_path in ept_files:\n",
    "        try:\n",
    "            tag = extract_tag_from_ept(ept_path)\n",
    "            ept_by_tag[tag] = ept_path\n",
    "        except ValueError as e:\n",
    "            print(f\"[WARN] {e}\")\n",
    "            continue\n",
    "\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for eval_path in eval_files:\n",
    "        try:\n",
    "            tag = extract_tag_from_eval(eval_path)\n",
    "        except ValueError as e:\n",
    "            print(f\"[WARN] {e}\")\n",
    "            continue\n",
    "\n",
    "        ept_path = ept_by_tag.get(tag)\n",
    "        if ept_path is None:\n",
    "            print(f\"[WARN] No matching EPT file for eval {eval_path.name} with tag {tag}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- Eval side ---\n",
    "        eval_dict      = load_eval_json_maybe_zip(eval_path)\n",
    "        student_scores = extract_main_task_scores(eval_dict)\n",
    "        norm_val, norm_kind = get_normalizer(eval_dict)\n",
    "\n",
    "        om_perf = compute_OM_perf(student_scores, teacher_scores, tasks=TASKS)\n",
    "\n",
    "        # --- EPT side ---\n",
    "        with ept_path.open(\"r\") as f:\n",
    "            ept = json.load(f)\n",
    "\n",
    "        E_run_J  = ept[\"E_run_J\"]\n",
    "        T_in     = ept.get(\"T_in\")\n",
    "        T_out    = ept.get(\"T_out\")\n",
    "        EPT_in_J_per_tok   = ept.get(\"EPT_in_J_per_tok\")\n",
    "        EPT_out_J_per_tok  = ept.get(\"EPT_out_J_per_tok\")\n",
    "        E_per_unit_J       = ept[\"EPT_total_J_per_tok\"]  # J/token\n",
    "\n",
    "        energy_ratio_T_over_S = teacher_E_per_unit_J / E_per_unit_J\n",
    "        Eff_model = om_perf * energy_ratio_T_over_S\n",
    "\n",
    "        rows.append({\n",
    "            \"kd_type\": kd_name,\n",
    "            \"model_name\": eval_path.stem,\n",
    "            \"eval_file\": str(eval_path),\n",
    "            \"ept_file\": str(ept_path),\n",
    "            \"tag\": tag,\n",
    "            \"normalizer_value\": norm_val,\n",
    "            \"normalizer_kind\": norm_kind,\n",
    "            \"OM_perf\": om_perf,\n",
    "            \"E_run_J\": E_run_J,\n",
    "            \"T_in\": T_in,\n",
    "            \"T_out\": T_out,\n",
    "            \"EPT_in_J_per_tok\": EPT_in_J_per_tok,\n",
    "            \"EPT_out_J_per_tok\": EPT_out_J_per_tok,\n",
    "            \"E_per_unit_J\": E_per_unit_J,\n",
    "            \"energy_ratio_T_over_S\": energy_ratio_T_over_S,\n",
    "            \"Eff_model\": Eff_model,\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.sort_values(\"Eff_model\", ascending=False, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Build dataframes for each KD family\n",
    "relation_df = build_kd_df(REL_EVAL_DIR, REL_EPT_DIR, \"relation\")\n",
    "response_df = build_kd_df(RESP_EVAL_DIR, RESP_EPT_DIR, \"response\")\n",
    "feature_df  = build_kd_df(FEAT_EVAL_DIR, FEAT_EPT_DIR, \"feature\")\n",
    "\n",
    "relation_df, response_df, feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- KD Matching Sanity Check ----\n",
    "\n",
    "def sanity_check_kd_pairing(eval_dir: Path, ept_dir: Path, kd_name: str):\n",
    "    print(f\"\\n=== Sanity Check for {kd_name.upper()} KD ===\")\n",
    "    \n",
    "    eval_files = sorted(eval_dir.glob(\"*.json\"))\n",
    "    ept_files  = sorted(ept_dir.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(eval_files)} eval files and {len(ept_files)} EPT files\")\n",
    "    \n",
    "    # Build map tag -> ept path\n",
    "    ept_map = {}\n",
    "    for e in ept_files:\n",
    "        try:\n",
    "            tag = extract_tag_from_ept(e)\n",
    "            ept_map[tag] = e\n",
    "        except Exception as ex:\n",
    "            print(f\"[WARN] Could not parse EPT file {e.name}: {ex}\")\n",
    "\n",
    "    rows = []\n",
    "    for ev in eval_files:\n",
    "        try:\n",
    "            tag = extract_tag_from_eval(ev)\n",
    "        except Exception as ex:\n",
    "            print(f\"[WARN] Could not parse eval file {ev.name}: {ex}\")\n",
    "            continue\n",
    "        \n",
    "        matched = ept_map.get(tag)\n",
    "        rows.append({\n",
    "            \"kd_type\": kd_name,\n",
    "            \"eval_file\": ev.name,\n",
    "            \"ept_file\": matched.name if matched else \"❌ NO MATCH\",\n",
    "            \"tag\": tag,\n",
    "        })\n",
    "    \n",
    "    df_check = pd.DataFrame(rows)\n",
    "    display(df_check)\n",
    "    \n",
    "    missing = df_check[df_check[\"ept_file\"] == \"❌ NO MATCH\"]\n",
    "    if not missing.empty:\n",
    "        print(\"\\n❌ Missing matches detected:\")\n",
    "        display(missing)\n",
    "    else:\n",
    "        print(\"\\n✅ All eval files correctly matched to EPT files!\")\n",
    "    \n",
    "    return df_check\n",
    "\n",
    "\n",
    "check_relation = sanity_check_kd_pairing(REL_EVAL_DIR, REL_EPT_DIR, \"relation\")\n",
    "check_response = sanity_check_kd_pairing(RESP_EVAL_DIR, RESP_EPT_DIR, \"response\")\n",
    "check_feature  = sanity_check_kd_pairing(FEAT_EVAL_DIR, FEAT_EPT_DIR, \"feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995279df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kd_df(eval_dir: Path, ept_dir: Path, kd_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a dataframe of OM_perf, energy-per-token, and Eff_model\n",
    "    for all student models in a given KD family (relation / response / feature).\n",
    "\n",
    "    We treat eval, telemetry, and token/EPT files as separate:\n",
    "      - eval_dir: LM-eval JSON files (metrics per task)\n",
    "      - ept_dir:  EPT/token JSON files (E_run_J, T_in, T_out, EPT_*_J_per_tok)\n",
    "    Raw telemetry JSONLs can live separately (e.g., REL_TELEM_DIR) but are\n",
    "    not needed for Eff_overall because E_run_J already appears in the EPT files.\n",
    "    \"\"\"\n",
    "    eval_files = sorted(eval_dir.glob(\"*.json\"))\n",
    "    if not eval_files:\n",
    "        print(f\"[WARN] No eval JSON files found in {eval_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for eval_path in eval_files:\n",
    "        stem = eval_path.stem\n",
    "        ept_path = ept_dir / f\"{stem}.json\"\n",
    "\n",
    "        if not ept_path.exists():\n",
    "            print(f\"[WARN] No EPT/token file for {eval_path.name}, expected {ept_path.name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- Eval side: scores + normalizer (tokens or seconds if present) ---\n",
    "        eval_dict      = load_eval_json_maybe_zip(eval_path)\n",
    "        student_scores = extract_main_task_scores(eval_dict)\n",
    "        norm_val, norm_kind = get_normalizer(eval_dict)\n",
    "\n",
    "        # OM_perf across the 4 tasks\n",
    "        om_perf = compute_OM_perf(student_scores, teacher_scores, tasks=TASKS)\n",
    "\n",
    "        # --- EPT/token side: energy per token ---\n",
    "        with ept_path.open(\"r\") as f:\n",
    "            ept = json.load(f)\n",
    "\n",
    "        E_run_J  = ept[\"E_run_J\"]\n",
    "        T_in     = ept.get(\"T_in\")\n",
    "        T_out    = ept.get(\"T_out\")\n",
    "        EPT_in_J_per_tok   = ept.get(\"EPT_in_J_per_tok\")\n",
    "        EPT_out_J_per_tok  = ept.get(\"EPT_out_J_per_tok\")\n",
    "        E_per_unit_J       = ept[\"EPT_total_J_per_tok\"]  # J per token\n",
    "\n",
    "        # Teacher / Student energy-per-token ratio\n",
    "        energy_ratio_T_over_S = teacher_E_per_unit_J / E_per_unit_J\n",
    "\n",
    "        # Overall efficiency for this model:\n",
    "        # high if (a) performance is close to teacher, and (b) student uses\n",
    "        # fewer Joules per token than the teacher.\n",
    "        Eff_model = om_perf * energy_ratio_T_over_S\n",
    "\n",
    "        rows.append({\n",
    "            \"kd_type\": kd_name,\n",
    "            \"model_name\": stem,\n",
    "            \"eval_file\": str(eval_path),\n",
    "            \"ept_file\": str(ept_path),\n",
    "            \"normalizer_value\": norm_val,\n",
    "            \"normalizer_kind\": norm_kind,   # \"tokens\" or \"seconds\" from eval harness (if available)\n",
    "            \"OM_perf\": om_perf,\n",
    "            \"E_run_J\": E_run_J,\n",
    "            \"T_in\": T_in,\n",
    "            \"T_out\": T_out,\n",
    "            \"EPT_in_J_per_tok\": EPT_in_J_per_tok,\n",
    "            \"EPT_out_J_per_tok\": EPT_out_J_per_tok,\n",
    "            \"E_per_unit_J\": E_per_unit_J,\n",
    "            \"energy_ratio_T_over_S\": energy_ratio_T_over_S,\n",
    "            \"Eff_model\": Eff_model,\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.sort_values(\"Eff_model\", ascending=False, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Build dataframes for each KD family\n",
    "relation_df = build_kd_df(REL_EVAL_DIR, REL_EPT_DIR, \"relation\")\n",
    "response_df = build_kd_df(RESP_EVAL_DIR, RESP_EPT_DIR, \"response\")\n",
    "feature_df  = build_kd_df(FEAT_EVAL_DIR, FEAT_EPT_DIR, \"feature\")\n",
    "\n",
    "relation_df, response_df, feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Eff_overall for each KD type\n",
    "if relation_df is None or relation_df.empty:\n",
    "    raise RuntimeError(\"No relation KD runs processed. Check eval / EPT paths and naming.\")\n",
    "if response_df is None or response_df.empty:\n",
    "    print(\"[WARN] No response KD runs processed.\")\n",
    "if feature_df is None or feature_df.empty:\n",
    "    print(\"[WARN] No feature KD runs processed.\")\n",
    "\n",
    "Eff_overall_relation = relation_df[\"Eff_model\"].mean()\n",
    "print(f\"Eff_overall (Relation KD): {Eff_overall_relation:.4f}\")\n",
    "\n",
    "Eff_overall_response = None\n",
    "Eff_overall_feature  = None\n",
    "\n",
    "if not response_df.empty:\n",
    "    Eff_overall_response = response_df[\"Eff_model\"].mean()\n",
    "    print(f\"Eff_overall (Response KD): {Eff_overall_response:.4f}\")\n",
    "\n",
    "if not feature_df.empty:\n",
    "    Eff_overall_feature = feature_df[\"Eff_model\"].mean()\n",
    "    print(f\"Eff_overall (Feature KD): {Eff_overall_feature:.4f}\")\n",
    "\n",
    "# Optional: combined view of all models from all KD families\n",
    "all_kd_df = pd.concat(\n",
    "    [df for df in [relation_df, response_df, feature_df] if df is not None and not df.empty],\n",
    "    ignore_index=True\n",
    ")\n",
    "all_kd_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
