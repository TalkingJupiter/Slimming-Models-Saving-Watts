{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9192a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CONFIG: which metric to use for each task, and from where in JSON\n",
    "# ------------------------------------------------------------------\n",
    "# In our JSON:\n",
    "# - arc_challenge:      results[\"arc_challenge\"][\"acc,none\"]\n",
    "# - hellaswag:          results[\"hellaswag\"][\"acc,none\"]\n",
    "# - bbh (group):        groups[\"bbh\"][\"exact_match,get-answer\"]\n",
    "# - mmlu (group):       groups[\"mmlu\"][\"acc,none\"]\n",
    "TASKS = [\"arc_challenge\", \"hellaswag\", \"bbh\", \"mmlu\"]\n",
    "\n",
    "METRIC_CONFIG = {\n",
    "    \"arc_challenge\": (\"results\", \"acc,none\"),\n",
    "    \"hellaswag\":     (\"results\", \"acc,none\"),\n",
    "    \"bbh\":           (\"groups\",  \"exact_match,get-answer\"),\n",
    "    \"mmlu\":          (\"groups\",  \"acc,none\"),\n",
    "}\n",
    "\n",
    "def load_json(path):\n",
    "    path = Path(path)\n",
    "    with path.open(\"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_task_scores(eval_json_path, tasks=TASKS, metric_cfg=METRIC_CONFIG):\n",
    "    \"\"\"\n",
    "    Return a dict: {task_name: score} for the given eval json.\n",
    "    \"\"\"\n",
    "    data = load_json(eval_json_path)\n",
    "    scores = {}\n",
    "    for task in tasks:\n",
    "        section_name, metric_key = metric_cfg[task]\n",
    "        section = data[section_name]\n",
    "        scores[task] = section[task][metric_key]\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585980c",
   "metadata": {},
   "source": [
    "## OM_perf — Overall Performance Metric\n",
    "\n",
    "We define the average performance of a KD method across **5 student models** and **4 tasks** using the following formula:\n",
    "\n",
    "\\[\n",
    "\\text{OM\\_perf}\n",
    "=\n",
    "\\frac{1}{N_{\\text{models}}}\n",
    "\\sum_{i=1}^{N_{\\text{models}}}\n",
    "\\left(\n",
    "    \\frac{1}{T}\n",
    "    \\sum_{t=1}^{T}\n",
    "    \\frac{s_{i,t}}{s^{(\\text{teacher})}_t}\n",
    "\\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(T = 4\\) — number of evaluation tasks  \n",
    "- \\(N_{\\text{models}} = 5\\) — number of distilled student models  \n",
    "- \\(s_{i,t}\\) — score of student model \\(i\\) on task \\(t\\)  \n",
    "- \\(s^{(\\text{teacher})}_t\\) — score of the teacher model on task \\(t\\)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Task averaging:**  \n",
    "   For each student model \\(i\\), we compute the average of the student/teacher accuracy ratios across all 4 tasks.\n",
    "\n",
    "2. **Model averaging:**  \n",
    "   We then average these per-model scores across all 5 KD student models.\n",
    "\n",
    "This gives a single scalar value measuring **how well the KD method performs relative to the teacher**, averaged across all tasks and student runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# PATHS you need to customize:\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Single teacher eval JSON\n",
    "TEACHER_EVAL = \"Base/eval/results/harness_meta-llama_Llama-3.1-70B-Instruct_20251113_144303_2025-11-14T01-20-58.355548.json\"   # <- Base teacher evaluation path\n",
    "\n",
    "# All 5 KD eval JSONs for ONE KD method (e.g., response KD)\n",
    "# Example: use glob if stored in a folder\n",
    "KD_EVAL_GLOB = \"evals/resp_kd/*.json\"   # <- change to your folder pattern\n",
    "\n",
    "kd_eval_paths = sorted(glob.glob(KD_EVAL_GLOB))\n",
    "print(\"KD eval files:\", kd_eval_paths)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load teacher scores\n",
    "# ------------------------------------------------------------------\n",
    "teacher_scores = extract_task_scores(TEACHER_EVAL, TASKS, METRIC_CONFIG)\n",
    "print(\"Teacher scores per task:\")\n",
    "for t in TASKS:\n",
    "    print(f\"  {t:15s}: {teacher_scores[t]:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load KD scores for each model\n",
    "# ------------------------------------------------------------------\n",
    "kd_models_scores = []\n",
    "for p in kd_eval_paths:\n",
    "    scores = extract_task_scores(p, TASKS, METRIC_CONFIG)\n",
    "    kd_models_scores.append((p, scores))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Compute OM_perf per model and overall average\n",
    "# ------------------------------------------------------------------\n",
    "om_perf_per_model = []  # list of floats\n",
    "\n",
    "for path, scores in kd_models_scores:\n",
    "    ratios = []\n",
    "    for t in TASKS:\n",
    "        s_kd = scores[t]\n",
    "        s_teacher = teacher_scores[t]\n",
    "        ratios.append(s_kd / s_teacher)\n",
    "    om_model = sum(ratios) / len(TASKS)     # average over tasks\n",
    "    om_perf_per_model.append(om_model)\n",
    "    print(f\"OM_perf for model {Path(path).name}: {om_model:.4f}\")\n",
    "\n",
    "om_perf_overall = sum(om_perf_per_model) / len(om_perf_per_model)\n",
    "print(\"\\n========================================\")\n",
    "print(f\"Overall OM_perf (KD method, avg over 5 models): {om_perf_overall:.4f}\")\n",
    "print(\"========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for (path, scores), om_val in zip(kd_models_scores, om_perf_per_model):\n",
    "    row = {\"model_json\": Path(path).name, \"OM_perf\": om_val}\n",
    "    # also store raw task scores if you want\n",
    "    for t in TASKS:\n",
    "        row[f\"{t}_score\"] = scores[t]\n",
    "        row[f\"{t}_ratio\"] = scores[t] / teacher_scores[t]\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)\n",
    "\n",
    "print(\"Overall OM_perf:\", om_perf_overall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfaba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
